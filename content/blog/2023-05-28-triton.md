---
title: "Beyond FastAPI: Using Nvidia Triton for serving ML models"
draft: false
date: "2023-05-28"
tags:
  - 
---
# Beyond FastAPI: Using Nvidia Triton for serving ML models


## Serving Models

In today's world, ML engineers are taking on the role of "full stack" professionals, not only developing new models but also deploying them. Python has emerged as the language of choice in the field of Machine Learning, leading engineers to gravitate towards Python-based web frameworks for model deployment. While FastAPI, a modern and high-performance web framework for building APIs in Python, has become a popular choice, it does have certain limitations when it comes to serving ML models effectively.

---

## The Limitations of FastAPI

FastAPI, with its support for asynchronous request processing and data validation, empowers ML engineers to write REST API endpoints for ML inference. However, it wasn't explicitly designed to serve as a dedicated inference engine. As a result, it lacks built-in support for ML frameworks and inference-specific features that can maximize throughput. Although FastAPI is ok for scenarios where high throughput isn't critical, as ML inference scales, it becomes advisable to transition to an inference engine that is explicitly built for serving ML models to maximize performance and efficiency.

## Introducing Nvidia Triton: A Powerful Inference Serving Platform

Nvidia Triton is an open-source inference serving platform that excels in optimizing and scaling machine learning model deployment. It surpasses FastAPI in several key aspects, making it an ideal solution for serving ML models efficiently:

- **Framework Support**: Triton supports all major ML frameworks, including Tensorflow, PyTorch, TensorRT, Onnx, and more. This built-in framework support simplifies the deployment process for models developed using various ML frameworks.

- **Dynamic Batching**: Triton can batch incoming requests in an online fashion, maximizing throughput and reducing latency. This efficient handling of multiple inference requests simultaneously enhances performance.

- **Concurrent Model Execution**: Triton enables loading multiple models and instances into GPU memory. These models can be executed in parallel, further boosting throughput and performance.

- **GPU Accelerated Preprocessing**: Triton leverages the power of GPUs with the NVIDIA Data Loading Library (DALI) for faster data transformation and enhanced inference speed.

- **Integration with Kubernetes and Prometheus**: Triton seamlessly integrates with Kubernetes for efficient container orchestration, enabling scalable deployment and management of ML models. Its integration with Prometheus facilitates monitoring and observability, essential for ML operations.

However, triton does have some drawbacks:

- **Large Docker Image Size**: Triton's Docker images can be large (~10.6 GB), even when deploying a Triton server with specific backends. This can result in increased storage and network requirements.
- **Learning Curve**: Setting up and configuring the Triton server may require some additional time and effort due to its advanced features and flexibility.

While these disadvantages should be taken into account, they are outweighed by the benefits when high throughput, scalability, and efficiency are the priority.

To learn more about Nvidia Triton, you can visit the [official website](https://developer.nvidia.com/nvidia-triton-inference-server).

---
## Example Repository: Deploying PyTorch Model for Bee vs. Ant Classification with Triton

To demonstrate the deployment workflow of ML models using NVIDIA Triton, FastAPI, and Gradio, I have created the example repository called [**triton-fastapi-docker**](https://github.com/jayeshmahapatra/triton-fastapi-docker). This repository provides a practical implementation of utilizing Triton with Docker containers to deploy a PyTorch-based machine learning model capable of distinguishing between pictures of bees and ants.

The [**triton-fastapi-docker**](https://github.com/jayeshmahapatra/triton-fastapi-docker) repository comprises the following services:

1. [triton](https://github.com/jayeshmahapatra/triton-fastapi-docker/tree/9f05a40b6566554cde428261c4059477854b6940/triton): This service runs the NVIDIA Triton inference engine, serving the PyTorch ML model that distinguishes between pictures of bees and ants.
2. [fastapi](https://github.com/jayeshmahapatra/triton-fastapi-docker/tree/9f05a40b6566554cde428261c4059477854b6940/fastapi): The Python backend communicates with `triton` service via gRPC, handling client requests effectively.
3. [gradio](https://github.com/jayeshmahapatra/triton-fastapi-docker/tree/9f05a40b6566554cde428261c4059477854b6940/gradio): This service provides a user-friendly interface using Gradio, allowing users to upload images for inference and interact with the inference pipeline seamlessly.

By following the instructions provided in the repository's [README](https://github.com/jayeshmahapatra/triton-fastapi-docker/blob/9f05a40b6566554cde428261c4059477854b6940/README.md), users can easily set up and deploy their own PyTorch-based ML model. Furthermore, the repository includes a Jupyter notebook located in the [`notebooks`](https://github.com/jayeshmahapatra/triton-fastapi-docker/tree/9f05a40b6566554cde428261c4059477854b6940/notebooks) directory, offering a step-by-step guide on creating a TorchScript model specifically designed to distinguish between pictures of bees and ants.

I hope this blog helped you consider triton for your future model deployments, and the repo serves as a valuable example for the same ! 